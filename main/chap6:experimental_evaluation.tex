\chapter{Experimental Evaluation}
\label{chap:experimental_evaluation}

In this chapter we first describe the datasets on which we conducted the experiments, and establish an objective explanation quality measure. We then present and analyze results of our approximate lineage system experimentation.


\section{Experimental Setup}\label{sec:evaluation-setup}
We used two datasets from different domains (food and movies) to compare the performance of our approximate lineage system against ProvSQL. The goal of these experiments is evaluating the precision of our approximate explanations against an ``exact provenance tracking system".
\par\textbf{Note.} \textit{max\_vectors\_num} (see section \ref{sec:proposed_solution_approx}) was chosen manually to be 4. The choice of hyperparameters, and, specifically, \textit{max\_vectors\_num} - is the subject of ongoing research.\\

% USDA BFPD and MovieLens
\subsection{Datasets}\label{sec:datasets}
\begin{enumerate}
    \item The USDA Branded Food Products Database (BFPDB) \cite{usda_bfpd-dataset} is the result of a Public-Private Partnership, whose goal is to enhance public health and the sharing of open data by complementing USDA Food Composition Databases with nutrient composition of branded foods and private label data, provided by the food industry. Among others, the dataset includes three tables:
    \begin{itemize}
        \item \texttt{products} - contains basic information on branded products (manufacturer, name, ingredients).
        \item \texttt{nutrients} - each tuple contains all nutrient value information about a product.
        \item \texttt{serving\_size} - each tuple contains information about a product's serving size (as well as unit of measurement) and food preparation state.
        \item \texttt{derivation\_code\_description} - contains all nutrient derivation codes and their detailed descriptions.
    \end{itemize}
    Each product in the BFPDB dataset has a unique identifier, \texttt{ndb\_no}. For a detailed description of the BFPDB dataset see appendix \ref{appendix:bfpdb_schema}.
    
    \item The MovieLens dataset \cite{movielens-dataset} describes people’s preferences for movies. These preferences are entered by way of the MovieLens website \cite{movielens-recommender-website} — a recommender system that asks its users to provide movie ratings in order to receive personalized movie recommendations. Among others, the dataset includes three tables:
    \begin{itemize}
        \item \texttt{movies} - each tuple contains basic information about a movie (title, genres).
        \item \texttt{ratings} - each tuple contains information about a user's expressed preference of a movie (a 0-5 star rating).
        \item \texttt{tags} - each tuple contains information about a user's classification tag of a movie (silly, funny, etc.).
    \end{itemize}
    Each movie in the MovieLens dataset has a unique identifier, \texttt{movieId}. For a detailed description of the MovieLens dataset see appendix \ref{appendix:movielens_schema}.
\end{enumerate}

% Explain how the precision evaluation process works (test against lineage from provsql)
\subsection{Precision Calculation}\label{sec:precision}
In section \ref{sec:proposed_solution_approx} we stated that we expect our explanations to approximate the exact lineage of query result tuples. 
Thus, in order to test the aforementioned algorithms and implementation, we devised an explanation quality measure for explaining a single tuple $t$, where $n$ is a parameter: 
\begin{equation*}
    \operatorname{Precision}(t, n) = \frac{|ApproxLineage(t, n) \cap ExactLineage(t)|}{|ApproxLineage(t, n)|}\\
\end{equation*}
where $ApproxLineage(t, n)$ is the set of the top $n$ (by lineage similarity) tuples, returned as explanations (i.e., approximate lineage) for $t$ by our system. $ExactLineage(t)$ is the set of tuples that comprise the exact \textbf{distant} lineage of $t$, it can be calculated recursively from the semiring polynomial that is returned by the ProvSQL system for $t$.
For example, if the direct lineage of $t_4$ and $t_5$ are the sets $\{t_1, t_2, t_3\}$ and $\{t_1\}$, respectively, and the direct lineage of a tuple $t_6$ is the set $\{t_4, t_5\}$ then the total distant lineage for the tuple $t_6$ is $\{t_1, t_2, t_3, t_4, t_5\}$.
The parameter $n$ is set by us in each experiment, that is, it is a parameter of the experiment. 
$\operatorname{Precision(t, n)}$ is tunable via the parameter $n$, i.e., when $n$ is small we test for precision of the ``top" explanations we found.\\
\par Classically, we are usually interested in precision and recall (e.g., in traditional statistics and classification problems). However, here the situation is slightly different, what we really are interested in is to assess the quality of our explanations by measuring ``how many of the top $n$ (by lineage similarity) tuples are actually part of the exact lineage?". By contrast, traditional recall does not seem to be a meaningful metric in our case, as many query-result tuples might have long histories. Hence, a top-N justifications result is preferred over ``returning all the correct lineage tuples", and this is what we measure.

\subsection{Distant Lineage: Per-Level Recall}\label{sec:recall}
    In order to assess the quality of a distant lineage answer in a more insightful way, we save the ``exact distant lineage" in a hierarchical structure, and analyze it per-level. An implementation is a list $L$ of sets per-tuple, s.t. $L[i]$ is the set of all lineage tuples at the $i^{th}$ derivation depth level. Note that $i$ starts at $0$, s.t. the $0^{th}$ lineage level is a set containing the target tuple only. For example, if the direct lineage of $t_4$ and $t_5$ are the sets $\{t_1, t_2, t_3\}$ and $\{t_1\}$, respectively, and the direct lineage of a tuple $t_6$ is the set $\{t_3, t_4, t_5\}$ (note that $t_3$ appears in both the $1^{st}$ and the $2^{nd}$ lineage levels of $t_6$, in this example) then the hierarchical lineage DAG for the tuple $t_6$, and the hierarchical list structure that represents it, look as follows: \\\\
    \input{figures/distant_lineage_tree} \hspace{35pt}
    \input{figures/distant_lineage_list} \\\\
Here, each rectangle is an entry in the list $L$.
\par We expect our explanations to have a ``natural ranking" property in terms of lineage levels. That is, we expect the similarity between a target tuple and tuples in its distant lineage to be inversely proportional to the distance between them in the hierarchical lineage DAG structure. Thus, we devise an explanation quality measure for the explanation of a single tuple $t$ and its lineage level $i$:
\begin{equation*}
    \operatorname{Recall}(t, i) = \frac{|ApproxLineage(t, D(t, i)) \cap L_t[i])|}{|L_t[i]|}\\
\end{equation*}
where $ApproxLineage(t, D(t, i))$ is the set of the top $D(t, i)$ (by lineage similarity) tuples, returned as explanations (i.e., approximate lineage) for $t$ by our system.
$D(t, i)$ is the number of unique returned tuples in the exact lineage of $t$ up until the $i^{th}$ level (including). Formally, it is defined as follows: $D(t, i) = |\bigcup\limits_{j=1}^{i} L_t[j]|$.
$L_t$ is the hierarchical list structure for $t$, as defined above, i.e., $L_t[i]$ is the set of all lineage tuples at the $i^{th}$ derivation depth level of $t$.\\

\section{Examples}\label{sec:initial-experimentation}

Having established an explanation quality measure, we conducted a series of experiments. In particular, we tested the precision of direct provenance of different queries, varying from ``simple" (examples \ref{experiment:1} and \ref{experiment:2}) to ``complex" (example \ref{experiment:3}) queries. We show some of the experimental results and present a qualitative analysis.
\par\textbf{Note.} In all the examples, the lineage (and its size - `Lin. size' column) of a tuple, is calculated as a subset of the exact lineage. That is, if we compare a query result tuple with $\texttt{products}.tuples$, i.e., all the tuples in the \texttt{products} table (in order to find the top $N$ most similar tuples) - we use only \texttt{product} tuples in the exact lineage for precision calculations ($Lineage\_products(t)$ = $ExactLineage(t) \cap \texttt{products}.tuples$).


\begin{runexample}
We test queries on a small subset of the MovieLens dataset. Each table (i.e., \texttt{movies}, \texttt{ratings}, \texttt{tags}) consists of about 10,000 tuples.
\end{runexample}
%%% Present here all the queries? Figure? I think NO

% Example 1
% MovieLens tuples vs. columns.
% \input{tables/example1}
% "Toy" example
\begin{example-withrun}\label{experiment:1}
\input{tables/example1}
\footnotetext{$Lineage\_tags(t) = ExactLineage(t) \cap \texttt{tags}.tuples$.}
\input{tables/example2}
\input{tables/example3}
We ask for distinct \texttt{users} that tagged a Comedy/Romance genre \texttt{movie} as one of \textit{funny}, \textit{superhero}, \textit{family}, \textit{music}. The results are shown in Table \ref{tab:1}. Precision calculation is discussed in section \ref{sec:evaluation-setup}. The approximate lineage is calculated from comparing result tuples to tuples from the \texttt{tags} table (see preceding Note). Right away, it is evident that provenance column vectors produce better approximate lineages than tuple vectors.\\
\par In this example tuples No.\texttt{0} and No.\texttt{6} display superior precision for column vectors. Tuple No.\texttt{7} is  interesting as it is a case of lineage size $>$ 1 for which both methods (tuple and column vectors) produce an accuracy of 1.0.
It is interesting to examine tuple No.\texttt{7} more closely to gain a better grasp of the differences between tuple and column vectors.
Now, let us analyze the top \textbf{3} (by similarity) tuples from the \texttt{tags} table, which are returned by our system for tuple No.\texttt{7} in Table \ref{tab:1} (\texttt{userId}=282405, Lin. size=\textbf{2}) for both the tuple-based and column vectors-based methods:\\\\\\
\par\textbf{Tuple Vectors:}\\
\input{tables/res2}\\
\par\textbf{Column Vectors:}\\
\input{tables/res1}
\\
As shown in the two tables above, both methods give maximal similarity (top 2) for the correct tuples in the exact lineage. However, the ``column vectors" method produces better separation ($0.94-0.89 = 0.05$ vs. $0.88-0.86=0.02$) in terms of similarity between correct and incorrect lineage tuples. Another interesting point of distinction between the two methods is the 3$^{rd}$ most similar tuple returned:
\begin{itemize}
    \item The ``tuple vectors" method returns a tuple which describes a tag of a correct movie (Zootopia) by the correct user (282405) with an incorrect label - \textit{acceptance}.
    \item The ``column vectors" method returns a tuple which describes a tag of an incorrect movie (Coco - not a Romance/Comedy movie) by the correct user (282405) with a correct label - \textit{music}.
\end{itemize}
The reason for this dissimilarity in behavior might be the result of giving the \texttt{tag} column more weight when comparing lineage column vectors (as discussed in section \ref{sec:query-dependent-weighting}).

\end{example-withrun}


\begin{runexample}
We test queries on a small subset of the BFPDB dataset. Each table (\texttt{products}, \texttt{nutrients}, \texttt{serving\_size}) consists of about 10,000 tuples.
\end{runexample}

% Example 2
% BFPD tuples vs. columns.
% Examine top mistakes and discuss column weighting.
\begin{example-withrun}\label{experiment:2}
% \input{tables/example2}
We ask for distinct \texttt{manufacturers} of products that have \textit{cholesterol} value information in the \texttt{nutrients} table. Let us first look at the query:
\input{figures/q1}

Now, by parsing the query we conclude that the columns \texttt{manufacturer, ndb\_no, nutrient\_name} are the ``columns of interest" (discussed in section \ref{sec:query-dependent-weighting}). Thus, when comparing lineage column vectors of a query result tuple with another tuple from the DB - we give the columns of interest more weight. The results (and comparisons between the different techniques) are shown in Table \ref{tab:2}. The approximate lineage is calculated from comparing result tuples to tuples from the \texttt{nutrients} table. As evident from the presented results, incorporating query-based column weighting can significantly improve the precision of our system. 
\par\textbf{Note.} This example displays lineage querying for result tuples with more than just a few tuples in their exact lineage (i.e., up to two hundreds).

\end{example-withrun}

% Example 3
% BFPD tuples vs. columns vs. Bloom
% Explain why Bloom helps
\begin{example-withrun}\label{experiment:3}
% \input{tables/example3}
We ask for distinct \texttt{manufacturers} of products that have \textit{reeses} in their name and contain \textit{protein}
                         \texttt{nutrient} information.
                Also, these \texttt{manufacturers} must produce \textit{unprepared} \texttt{products} (this information is stored in the \texttt{serving\_size} table).
The only \texttt{manufacturer} in the BFPDB dataset that satisfies the (albeit a complex one) query is \textit{general mills sales}. Moreover, its exact lineage size (obtained by ProvSQL) is a relatively big number - \textbf{1966}; hence, it makes an interesting case study for the existence of drift errors in the construction of lineage vectors. That is, there is a potential for noise accumulation when combining a large number of lineage vectors with $+,\cdot$ and clustering operations (as discussed in section \ref{sec:proposed_solution_approx}). 
The results are shown in Table \ref{tab:3}. The approximate lineage is calculated from comparing result tuples to tuples from the \texttt{serving\_size} table. As evident from the results, our solution (with query-based weighted column vectors and Bloom filters of queries) does not suffer from drift errors (limiting drift errors for distant provenance and direct provenance at scale are the subjects of ongoing research).
\par\textbf{Notes.} 
\begin{itemize}
    \item The query-result tuple for \textit{general mills sales} has a lineage of size 1966, which is ${\sim}20\%$ of the tuples in the \texttt{serving\_size} table in our experiments (in a production setting, it can be orders of magnitude larger). Instead, we approximate the lineage with a small (and bounded) number of vectors. We argue that if lineage is huge our method makes more sense for direct provenance calculation than exact lineage calculations.
    \item In this example the query accesses three tables in a non-trivial way, which suggests the way more complex queries would behave.
\end{itemize}

\footnotetext{
$Lineage\_nutrients(t) = ExactLineage(t) \cap \texttt{nutrients}.tuples.$
}
\end{example-withrun}


\section{Advanced Experiments - The BFPDB Dataset}\label{sec:advanced_experiments}
% Explain what this section will contain - mainly, distant lineage testing
In this section we present results of our lineage tracking system over the USDA BFPDB dataset (see section \ref{sec:datasets}) on more elaborate and complex test scenarios. In particular, we focus on tuples with multiple generations in their lifelong lineage history and analyze them in terms of direct and \textbf{distant} lineage.
We assess the performance of our system quantitatively using precision and per-level recall (see sections \ref{sec:precision} and \ref{sec:recall}, respectively) and qualitatively by observing the ``top-$K$" returned lineage tuples (by similarity).
\par\textbf{Notes.}
\begin{itemize}
    \item Bloom-Filters of queries are not applicable to the querying of distant lineage, as they look for tuples that were not directly involved in the evaluation of a query. Hence, they are not used in the following experiments. Extending this basic method to handle indirect lineage is the subject of current research.
    \item Suggested improvements (chapter \ref{chap:improving_lineage_embeddings}) such as query-dependent column weighting, tuple creation timestamp and weighting with query dependency DAG are implicitly implemented (in the following experiments) for both the Tuple Vectors and the Column Vectors approximate lineage computation methods.
\end{itemize}


\subsection{A Hierarchy of Materialized Views}
% Present and explain the hierarchy - like in github but more in detail.
We want to simulate a DBMS that contains a significant portion of tuples that depend on the contents of the DB, as a platform for testing and analyzing distant lineage. Thus, we built a hierarchy of ``materialized views", such that each ``view" is a table, constructed from tuples that were generated by a single query. See Figure \ref{fig:materialized_views} for a diagram, depicting the hierarchy of our materialized views.
\input{figures/materialized_views}
\par As stated earlier, we consider those tables that do not depend on the contents of the DB when these tables are created, and thereafter, as \textit{base tables}. Again, note that tuples can be manually inserted to and deleted from base tables, but, not in a programmatic manner over the DB, e.g., a SQL \texttt{INSERT} statement that is based on the DB contents. In our case, the base tables are: \texttt{products}, \texttt{nutrients} and \texttt{serving\_size} (see section \ref{sec:datasets} for more details). Note that each materialized view depends \textit{directly} on tuples from the base tables (as defined above) or on tuples from other previously constructed materialized views.

% Also, talk about comparing to "related base tables" and "related materialized views" during the testing, as opposed to a single table.

\subsection{Experiments}
% Distant Lineage and fancy stuff
\begin{runexperiment}
We test queries on a subset of the BFPDB dataset, that consists of all the tables and materialized views depicted in Figure \ref{fig:materialized_views}. In the experiments we mimic an analyst's interaction with the data more precisely by comparing the approximate lineage vectors of a target tuple with a ``heterogeneous" group of tuples (e.g., all related base tables or all related materialized views) and ranking all the tuples among the group according to similarity scores, as opposed to dealing with one table at a time (as was previously shown during the initial experimentation of section \ref{sec:initial-experimentation}).
\end{runexperiment}

\begin{experiment-withrun}\label{advanced-experiment:1}
We ask for distinct \texttt{manufacturers} of products that have \textit{water}, \textit{sugar} and \textit{salt} as  ingredients, and contain \textit{protein} value information in the \texttt{nutrients} table; also, these manufacturers must produce \textit{prepared} and \textit{unprepared} products (this information is stored in the \texttt{serving\_size} table):\footnotemark
\input{figures/q_advanced_experiment_1}
\footnotetext{For a detailed description of $\texttt{exp}\sb\texttt{3}$ see section \ref{appendix:sec:exp3} in the appendix.}

Table \ref{tab:experiment1-base-tables:precision-and-recall} presents lineage related statistics, collected when computing the approximate lineage of the single result tuple in the output of this query (\textit{red gold}) against all the tuples from the related \textbf{base tables}: \texttt{products}, \texttt{nutrients} and \texttt{serving\_size}, which contain $29,322$ tuples in total. Tables \ref{tab:experiment1-base-tables:analyst-cv} and \ref{tab:experiment1-base-tables:analyst-tv} present the top 20 tuples from the related base tables, ranked by similarity, using the CV and TV methods, respectively. We make some observations by analyzing the results:
\begin{enumerate}
    \item The lineage sizes of the different levels (1-3) sum up to more than the total distant lineage size ($63 + 0 + 160 > 160$). In this case, it means that all 63 tuples from the $1^{st}$ lineage level (which contains tuples from the \texttt{products} relation) are also a part of the $3^{rd}$ lineage level (which contains tuples from the \texttt{products}, \texttt{nutrients} and \texttt{serving\_size} relations). From analyzing the query we may conclude that these are the 63 tuples from the \texttt{products} table that affect the query result directly (via the \texttt{FROM} clause) and indirectly (via tuples from $\texttt{exp}\sb\texttt{3}$, see a path from $\texttt{exp}\sb\texttt{3}$ to \texttt{products} in Figure \ref{fig:materialized_views}).
    \item The Column Vectors (CV) method demonstrates overall superiority, compared to the Tuple Vectors (TV) method. Moreover, the  CV method has a precision of 0.9 for the top $0.75\cdot160 = 120$ tuples in the approximate lineage, and 0.74 for the top 160 tuples. That is, most of the errors are produced for the lower ranked tuples (ranked 121, ..., 160).
    \item The CV method exhibits relatively high L[1] and L[3] recall scores (see Table \ref{tab:experiment1-base-tables:precision-and-recall}), evidenced by observing the Lineage Level(s) column in Table \ref{tab:experiment1-base-tables:analyst-cv}. That is, not only that all the top-20 tuples by similarity are really a part of the distant lineage, but also, the ranking preserves a level-based bias (most of the top-20 are in the $1^{st}$ lineage level). By contrast, the TV method is less impressive on this front, as is evidenced by the \textcolor{Red}{No} results in Table \ref{tab:experiment1-base-tables:analyst-tv}.
    \item Note that a random choice of the top 160 lineage tuples would have yielded a $\frac{160}{29,322} \approx 0.006$ precision score, which is several orders of magnitude worse than the scores of the CV and TV methods. 
\end{enumerate}

\input{tables/experiment1/base_tables_stats}

Table \ref{tab:experiment1-materialized-views:precision-and-recall} presents lineage related statistics, collected when computing the approximate lineage of the single result tuple in the output of this query (\textit{red gold}) against all the tuples from the related \textbf{materialized views}: $\texttt{exp}\sb\texttt{3}$, \texttt{protein}, \texttt{prepared} and \texttt{unprepared}, which contain $10,295$ tuples in total. Tables \ref{tab:experiment1-materialized-views:analyst-cv} and \ref{tab:experiment1-materialized-views:analyst-tv} present subsets of the top 90 and 100 (respectively) tuples from the related materialized views, ranked by similarity, using the CV and TV methods, respectively. We make some observations by analyzing the results:
\begin{enumerate}
    \item Both the CV and TV methods exhibit impressive outcomes, in terms of total precision and per-level recall, although the CV method performs better. That is, out of the top 85 tuples, only two are mistakenly ranked as lineage tuples (with a total of 83 tuples actually in the exact distant lineage), as can be seen in Table \ref{tab:experiment1-materialized-views:analyst-cv}.
    \item The top rated tuple, in both methods, is the only tuple in the $1^{st}$ lineage level (from $\texttt{exp}\sb\texttt{3}$). This somewhat supports our claim of level-based bias.
    \item The CV method showcases a significant separation between the similarity scores of the lowest ranked ``correct" lineage tuples (up to the $85^{th}$ tuple, including) and the next non-lineage one ($0.78 > 0.67$), see Table \ref{tab:experiment1-materialized-views:analyst-cv}. On the other hand, the TV method has no evident separation at all, as can be observed in Table \ref{tab:experiment1-materialized-views:analyst-tv} (see tuples 81-100).
\end{enumerate}

\input{tables/experiment1/base_tables_analyst-cv}
\input{tables/experiment1/base_tables_analyst-tv}
\input{tables/experiment1/materialized_views_stats}
\input{tables/experiment1/materialized_views_analyst-cv}
\input{tables/experiment1/materialized_views_analyst-tv}
\end{experiment-withrun}


\begin{experiment-withrun}\label{advanced-experiment:2}
We ask for distinct pairs of (\texttt{manufacturer}, \texttt{name}) for products that are listed in the $\texttt{exp}\sb\texttt{4}$ materialized view:\footnotemark
\input{figures/q_advanced_experiment_2}
\footnotetext{For a detailed description of $\texttt{exp}\sb\texttt{4}$ see section \ref{appendix:sec:exp4} in the appendix.}

Table \ref{tab:experiment2-base-tables:precision-and-recall} presents lineage related statistics, collected when computing the approximate lineage of the (two) result tuples in the output of this query ((\textit{campbell soup company, v8 beverage carrot mango}), (\textit{campbell soup company, v8 vfusion beverage peach mango})) against all the tuples from the related \textbf{base tables}: \texttt{products}, \texttt{nutrients} and \texttt{serving\_size}, which contain $29,322$ tuples in total. Tables \ref{tab:experiment2-base-tables:analyst-cv} and \ref{tab:experiment2-base-tables:analyst-tv} present the top 20 tuples from the related base tables, ranked by similarity to result tuple No.\texttt{0}, using the CV and TV methods, respectively. We make some observations by analyzing the results:
\begin{enumerate}
    \item The lineage sizes (of both result tuples) of the different levels (1-4) sum up to more than the total distant lineage sizes. E.g., looking at result tuple No.\texttt{0}: $2 + 0 + 4 + 80 > 82$. In this case, it means that at least two of the four tuples from the $3^{rd}$ lineage level (which contains tuples from the \texttt{products} and \texttt{serving\_size} relations, via $\texttt{exp}\sb\texttt{4}\longrightarrow\texttt{readytodrink}$, see Figure \ref{fig:materialized_views}) are also a part of either the $1^{st}$ lineage level (which contains tuples from the \texttt{products} relation, via the \texttt{FROM} clause), or the $4^{th}$ lineage level (which contains tuples from the \texttt{products}, \texttt{nutrients} and \texttt{serving\_size} relations).
    \item The Column Vectors (CV) method demonstrates overall superiority, compared to the Tuple Vectors (TV) method, topping at a precision score of 0.81 for the top 82 (total lineage size) approximate lineage tuples, for both result tuples.  Moreover, the TV method demonstrates unusually low  ($\approx0.2$) precision scores for this query.
    \item The TV method exhibits perfect L[1] and L[3] recall scores, and a low L[4] recall score, for both result tuples (see Table \ref{tab:experiment2-base-tables:precision-and-recall}). We conclude that the TV method is highly effective in finding the $1^{st}$ and $3^{rd}$ lineage-level tuples, but performs poorly in finding the $4^{th}$ lineage-level tuples, as is evidenced by the \textcolor{Red}{No} results in Table \ref{tab:experiment2-base-tables:analyst-tv}.
    \item The CV method exhibits relatively high L[1] and L[4] recall scores and a somewhat mediocre L[3] recall score for both result tuples (see Table \ref{tab:experiment2-base-tables:precision-and-recall}), evidenced by observing the Lineage Level(s) column in Table \ref{tab:experiment2-base-tables:analyst-cv}. That is, not only that almost all the top-20 tuples by similarity are really a part of the distant lineage, but also, the ranking preserves a level-based bias (the top-2 tuples are the only ones that appear both in the $1^{st}$, $3^{rd}$ and $4^{th}$ lineage levels).
    \item Analyzing the Tables \ref{tab:experiment2-base-tables:analyst-cv} and \ref{tab:experiment2-base-tables:analyst-tv}, we see that the two $3^{rd}$ lineage-level tuples from the \texttt{serving\_size} relation are discovered significantly earlier in the ranking by the TV method ($3^{rd}$ and $4^{th}$ tuples, by similarity) in comparison with the CV method ($80^{th}$ and $83^{rd}$ tuples, by similarity). This means that the TV method does a better job on the $3^{rd}$ lineage level, in terms of recall. This observation is backed up by the L[3] recall results we see in Table \ref{tab:experiment2-base-tables:precision-and-recall}.
    \item Notice the $4^{th}$ ranked tuple (by similarity) in Table \ref{tab:experiment2-base-tables:analyst-cv}, which is a \textcolor{Red}{No} lineage tuple, with a relatively high similarity score. A closer look reveals that this tuple is a product named \textit{v8 splash beverage mango peach}, which is also produced by \textit{campbell soup company}. Interestingly, it also appears as the first \textcolor{Red}{No} tuple when analyzing result tuple No.\texttt{1} vs. related base tables with the CV method (we do not show this explicitly).
    \item Note that a random choice of the top 82 lineage tuples would have yielded a $\frac{82}{29,322} \approx 0.003$ precision score, which is several orders of magnitude worse than the scores of the CV (and even the TV) methods. 
\end{enumerate}

\input{tables/experiment2/base_tables_stats}

Table \ref{tab:experiment2-materialized-views:precision-and-recall} presents lineage related statistics, collected when computing the approximate lineage of the (two) result tuples in the output of this query ((\textit{campbell soup company, v8 beverage carrot mango}), (\textit{campbell soup company, v8 vfusion beverage peach mango})) against all the tuples from the related \textbf{materialized views}: $\texttt{exp}\sb\texttt{4}$, $\texttt{exp}\sb\texttt{2}$, \texttt{protein}, \texttt{unprepared} and \texttt{readytodrink}, which contain $9,658$ tuples in total. Tables \ref{tab:experiment2-materialized-views:analyst-cv} and \ref{tab:experiment2-materialized-views:analyst-tv} present the top 20 tuples from the related materialized views, ranked by similarity to result tuple No.\texttt{0}, using the CV and TV methods, respectively. We make some observations by analyzing the results:
\begin{enumerate}
    \item The CV method demonstrates overall superiority, compared to the TV method, in terms of total precision and per-level recall. For example, for result tuple No.\texttt{0}, out of the top 44 tuples, only four are mistakenly ranked as lineage tuples (with a total of 40 tuples actually in the exact distant lineage), as can be partially seen in Table \ref{tab:experiment2-materialized-views:analyst-cv}.
    \item The top rated tuple in the CV method (Table \ref{tab:experiment2-materialized-views:analyst-cv}), is the only tuple in the $1^{st}$ lineage level (from $\texttt{exp}\sb\texttt{4}$). This somewhat supports our claim of level-based bias (the L[2] and L[3] recall scores are relatively high as well).
    \item By contrast, for result tuple No.\texttt{0}, the TV method ranks the only $1^{st}$ lineage-level tuple from $\texttt{exp}\sb\texttt{4}$ in $3^{rd}$ place, which is evidenced by the 0\footnotemark\, L[1] score in Table \ref{tab:experiment2-materialized-views:precision-and-recall} and can be seen in detail in Table \ref{tab:experiment2-materialized-views:analyst-tv}.
    \footnotetext{Following the definitions in section \ref{sec:recall}:\\
    $\operatorname{Recall}(t_0, 1) = \frac{|ApproxLineage(t_0, D(t_0, 1)) \cap L_{t_0}[1])|}{|L_{t_0}[1]|} = \frac{|ApproxLineage(t_0, |L_{t_0}[1]|) \cap L_{t_0}[1])|}{|L_{t_0}[1]|} = \frac{|ApproxLineage(t_0, 1) \cap L_{t_0}[1])|}{1} = |\{t_{\texttt{protein}}\} \cap \{t_{\texttt{$\texttt{exp}\sb\texttt{4}$}}\}| = |\emptyset| = 0$}
    \item Both the TV and CV methods rank the other, non-lineage tuple from $\texttt{exp}\sb\texttt{4}$, in the top 20, without a significant similarity score separation between the correct and the incorrect tuples from $\texttt{exp}\sb\texttt{4}$ (0.99 vs. 0.94 for the CV method, and 0.96 vs. 0.9 for the TV method). This suggests a relatively high similarity in terms of lineage, between the two tuples in the $\texttt{exp}\sb\texttt{4}$ materialized view (from transitivity).
\end{enumerate}

\input{tables/experiment2/base_tables_analyst-cv}
\input{tables/experiment2/base_tables_analyst-tv}
\input{tables/experiment2/materialized_views_stats}
\input{tables/experiment2/materialized_views_analyst-cv}
\input{tables/experiment2/materialized_views_analyst-tv}
\end{experiment-withrun}


\begin{experiment-withrun}\label{advanced-experiment:3}
We ask for distinct pairs of (\texttt{manufacturer}, \texttt{name}) for products that contain \textit{rice} in their name, and are produced by manufacturers of products that have \textit{water}, \textit{sugar} and \textit{salt} as  ingredients, and contain \textit{protein} value information in the \texttt{nutrients} table; also, these manufacturers must produce \textit{prepared} and \textit{unprepared} products (this information is stored in the \texttt{serving\_size} table):\footnotemark
\input{figures/q_advanced_experiment_3}
\footnotetext{For a detailed description of $\texttt{exp}\sb\texttt{2}$ see section \ref{appendix:sec:exp2} in the appendix.}

Table \ref{tab:experiment3-base-tables:precision-and-recall} presents lineage related statistics, collected when computing the approximate lineage of four (out of nine, for brevity) of the result tuples in the output of this query against all the tuples from the related \textbf{base tables}: \texttt{products}, \texttt{nutrients} and \texttt{serving\_size}, which contain $29,322$ tuples in total. Tables \ref{tab:experiment3-base-tables:analyst-cv} and \ref{tab:experiment3-base-tables:analyst-tv} present subsets of the top 100 and 20 (respectively) tuples from the related base tables, ranked by similarity to result tuple No.\texttt{2}, using the CV and TV methods, respectively. We make some observations by analyzing the results:
\begin{enumerate}
    \item This query presents a case where the size of the total distant lineage of result tuples is significant in terms of the input DB size. E.g., the total distant lineage size of result tuple No.\texttt{2} is $4,000$, while the related base tables contain a total of $29,322$ tuples ($\approx 1:10$).
    \item The Column Vectors (CV) method demonstrates overall superiority, compared to the Tuple Vectors (TV) method. Moreover, analyzing result tuple No.\texttt{2}, the CV method has a precision of 0.97 for the top $0.5\cdot4,000 = 2,000$ tuples in the approximate lineage, and 0.67 for the top $4,000$ tuples. That is, most of the errors are produced for the lower ranked tuples (ranked 2001, ..., 4000). These numbers are shown explicitly in Table \ref{tab:experiment3-base-tables:analyst-cv}, that shows the $52^{nd}$ ranked tuple as the only non-lineage tuple in the top 100.
    \item The TV method exhibits perfect L[1] and L[2] recall scores, and a low L[3] recall score, for all four result tuples (see Table \ref{tab:experiment3-base-tables:precision-and-recall}). We conclude that the TV method is highly effective in finding the $1^{st}$ and $2^{nd}$ lineage-level tuples, but performs poorly in finding the $3^{rd}$ lineage-level tuples, as is evidenced by the \textcolor{Red}{No} results in Table \ref{tab:experiment3-base-tables:analyst-tv}.
    \item The CV method exhibits perfect L[1] recall scores and relatively high L[2] and L[3] recall scores (see Table \ref{tab:experiment3-base-tables:precision-and-recall}). Analyzing result tuple No.\texttt{2} and Table \ref{tab:experiment3-base-tables:analyst-cv}, the 0.5 L[2] recall score seems a little misleading, since the $2^{nd}$ L[2] lineage-level tuple is not ``lost", but is ranked $3^{rd}$ (instead of $2^{nd}$). This somewhat supports our claim of level-based bias.
    \item Note that a random choice of the top $4,000$ lineage tuples would have yielded a $\frac{4,000}{29,322} \approx 0.136$ precision score, which is comparable with the scores of the TV method, but, is significantly worse than the CV method scores.
\end{enumerate}

\input{tables/experiment3/base_tables_stats}

Table \ref{tab:experiment3-materialized-views:precision-and-recall} presents lineage related statistics, collected when computing the approximate lineage of four (out of nine, for brevity) of the result tuples in the output of this query against all the tuples from the related \textbf{materialized views}: $\texttt{exp}\sb\texttt{2}$, \texttt{protein}, \texttt{sugars} and \texttt{unprepared}, which contain $10,298$ tuples in total. Tables \ref{tab:experiment3-materialized-views:analyst-cv} and \ref{tab:experiment3-materialized-views:analyst-tv} present the top 20 tuples from the related materialized views, ranked by similarity to result tuple No.\texttt{2}, using the CV and TV methods, respectively. We make some observations by analyzing the results:
\begin{enumerate}
    \item Once again, the CV method demonstrates overall superiority, compared to the TV method, in terms of total precision and per-level recall.
    \item Analyzing result tuple No.\texttt{2}, the top rated tuple in both the CV and TV methods (see Tables \ref{tab:experiment3-materialized-views:analyst-cv} and \ref{tab:experiment3-materialized-views:analyst-tv}, respectively) is the only \texttt{sugars} tuple in the $1^{st}$ lineage level. The $2^{nd}$ tuple in the $1^{st}$ (L[1]) lineage level (from $\texttt{exp}\sb\texttt{2}$) is ``discovered" earlier (ranked $3^{rd}$ vs. $5^{th}$) in the CV method, with no similarity score separation from the higher ranked tuples. By contrast, the TV method exhibits a significant similarity score drop all across the top 20 ranked tuples (0.97 for $1^{st}$ ranked vs. 0.81 for $5^{th}$ ranked vs. 0.64 for $20^{th}$ ranked). We view a more stable similarity score progression as indicating a better overall performance.
    \item Analyzing Tables \ref{tab:experiment3-materialized-views:precision-and-recall} and \ref{tab:experiment3-materialized-views:analyst-cv}, it seems the numbers for result No.\texttt{2} are not as good (compared to previous experiments and the result tuples No.\texttt{0} and No.\texttt{1} in this current experiment). A closer look reveals that out of the top 100 ranked tuples (by similarity) - all the mistakes are made on tuples from the \texttt{sugars} and \texttt{protein} materialized views. The \textcolor{Red}{No} tuples from \texttt{sugars} and \texttt{protein} are mostly products that appear also in the \texttt{unprepared} materialized view, and are actually in the lineage (i.e., \textcolor{Green}{Yes} tuples) of result tuple No.\texttt{2}. It seems that these \textcolor{Red}{No} tuples from \texttt{sugars} and \texttt{protein} have the same lineage vectors per the product related columns as the respective \textcolor{Green}{Yes} tuples from \texttt{unprepared}. Now, the Bloom Filters are not applicable here (since we are looking for distant lineage), and the Query Dependency DAG has no real filtering capabilities in this case (since the queries that created \texttt{sugars} and \texttt{protein} are relevant), hence we are not able to effectively get rid of these mistakes.

\end{enumerate}

\input{tables/experiment3/base_tables_analyst-cv}
\input{tables/experiment3/base_tables_analyst-tv}
\input{tables/experiment3/materialized_views_stats}
\input{tables/experiment3/materialized_views_analyst-cv}
\input{tables/experiment3/materialized_views_analyst-tv}
\end{experiment-withrun}


% \subsection{Where Provenance}
% MAYBE?
