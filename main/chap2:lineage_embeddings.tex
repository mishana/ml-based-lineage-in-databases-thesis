\chapter{Lineage via Embeddings}
\label{chap:lineage_embeddings}

\section{Word Embeddings}\label{sec:word_embeddings_intro}
% The need for word embeddings in NLP
Classic NLP research focuses on understanding the structure of text. For example, building dependency-based parse trees \cite{melcuk1988, klein-manning-2004-corpus} that represent the syntactic structure of a sentence via grammatical relations between the words. These approaches did not account for the \textbf{meaning} of words. \textit{Word embedding} aims to encode meanings of words (i.e., semantics), via low dimension (usually, 200-300) real-valued vectors, which can be used to compute the similarity of words as well as test for analogies \cite{DBLP:conf/naacl/MikolovYZ13}. Two of the most influential methods for computing word embeddings are the \textsc{Word2Vec} family of algorithms, by Mikolov et al. \cite{DBLP:journals/corr/abs-1301-3781, DBLP:conf/nips/MikolovSCCD13} and \textsc{GloVe} by Pennington et al. \cite{pennington2014glove}. Furthermore, applying neural-network (NN) techniques to NLP problems (machine translation \cite{wu2016googles}, named entity recognition \cite{Gillick_2016}, sentiment analysis \cite{Maas:2011:LWV:2002472.2002491} etc.) naturally leads to the representation of words and text as real-valued vectors. 

\section{Motivation}\label{sec:motivation_approx}
A few problems become apparent when considering \textit{Distant Lineage} (i.e., indirect, history long, explanations for the existence of tuples in the DB) with traditional and state of the art ``exact provenance tracking" techniques:
\begin{itemize}
    \item Formula based representations (e.g., semiring polynomials \cite{green2007provenance}) may blow-up in terms of space consumption.
    A naive implementation scenario using semiring polynomials requires saving the full provenance polynomial for each tuple. 
    This approach results in a massive growth in space consumption for storing those polynomials (for tuples that are produced by a query and that may depend on result tuples of previous queries).
    \item Inductively built representations (e.g., circuits \cite{Deutch2014, Senellart2017}) would become very complex over time. Thus, they result in impractical provenance querying time. A naive implementation scenario using circuits would simply keep on constructing provenance circuits as described in section \ref{sec:provenance circuits}. During lineage querying, we may end up with very complex circuits, such that numerous leaves are derived via a circuit of their own (these leaves are tuples that were produced by a previous query and were inserted to the DB). This implies that even if a significant amount of sharing is realized across the provenance circuits - these constructions are inevitably going to blow-up in space consumption. This approach renders keeping and querying the full provenance impractical and requires limiting heavily the provenance resolution, otherwise (e.g., return a summarized explanation). 
    \item Alternatively, if we only want lineage, rather than keeping a circuit, we could just store with each tuple a set of all the tuples it depends on. This will be cheaper than circuits but still prohibitively expensive. Here too, one could think of circuit-like techniques where tuples that have a subset in common, of tuples in their lineage, could share this subset. But again, this is complex and suffers from similar problems, as discussed above.
    
    \item Complex explanations are not very human-readable. Deutch et al. \cite{Deutch2017} showed how to generate more human-readable explanations - but they are arguably still complex. A ``top-N justifications" style provenance might be more useful for an analyst in a real-time interaction with the data. 
\end{itemize}


% \section{Proposed Solution}\label{sec:proposed_solution_approx}
% % Our proposal - encode provenance information using word embeddings
% \par We devise a novel approach to provenance tracking, which is based on ML and NLP techniques. The main idea is summarizing, and thereby approximating, the lineage of every tuple with a set of up to $max\_vectors\_num$ (a hyperparameter) vectors. For tuples that are inserted explicitly into the DB, the vectors are obtained using a pre-trained word embeddings model $M$ over the underlying ``text" of the tuple (see Figure \ref{algo:init_approx}).
% During a query execution process we form the lineage of new query result tuples in a similar fashion to that of provenance semirings \cite{green2007provenance}. We extend the + and $\cdot$ operations (see Figures \ref{algo:add_approx} and
% \ref{algo:mul_approx}, respectively) to generate lineage embeddings, while emphasizing the ability to propagate information and preserve the representation of lineage via a set of up to $max\_vectors\_num$, constant-size vectors. We obtain lineage embeddings for query output tuples by using this process. These new tuples (and their lineage) may be later inserted into the DB (depending on the specific application). 
% \par A real world provenance system like ProvSQL \cite{provsql_github} can make use of our lineage vectors during the construction of provenance circuits. Recall that each node in a provenance circuit is associated with some annotation/token, either representing a tuple from the input DB or an intermediate calculation (see section \ref{sec:provsql}). Lineage vectors can be incorporated in such a system by calling the Algorithm in Figure \ref{algo:provsql_online_approx}, for instance. This is an ``online" approach, meaning the lineage vectors are generated during the query execution process. Another approach to incorporating lineage vectors in a real world provenance system might be using such a system as a black box, generating a provenance how-formula for each query output tuple. Later, this formula can be analyzed ``offline", parsed, and converted to lineage vectors by evaluating the parsed expression in a hierarchical manner. Each intermediate evaluation step would call either Figure \ref{algo:add_approx} or \ref{algo:mul_approx}, based on the parsing of the formula, mimicking the online nature of algorithm \ref{algo:provsql_online_approx}.


% \begin{runexample}
%     % Show examples of addition and multiplication constructions
%     Next, we show examples of the addition and multiplication lineage vectors constructions (Figures \ref{algo:add_approx} and
% \ref{algo:mul_approx}, respectively).
%     Let $\vec v_1, \vec v_2, \vec v_3, \vec v_4 \in \mathbb{R}^2$ be vectors, such that:
%     \begin{equation*}
%         \vec v_1 = \begin{pmatrix} -1\\ 0.5 \end{pmatrix},
%         \vec v_2 = \begin{pmatrix} 1\\ 1 \end{pmatrix},
%         \vec v_3 = \begin{pmatrix} -0.5\\ 1 \end{pmatrix},
%         \vec v_4 = \begin{pmatrix} 0\\ -1 \end{pmatrix}
%     \end{equation*}
%     Suppose we have two tuples $t_1, t_2$ with respective sets of lineage vectors $LV_1, LV_2$, such that:
%     \begin{equation*}
%         LV_1 = \{\vec v_1, \vec v_2, \vec v_3\}, LV_2 = \{\vec v_4\}
%     \end{equation*}
%     Finally, the hyperparameters are:
%     \begin{equation*}
%         max\_vectors\_num = 3
%     \end{equation*}
% \end{runexample}
% % addition
% \begin{example-withrun}
%     Let us follow the construction of $LV_3$, which represents the lineage embeddings of $t_1 + t_2$ using the addition Algorithm in Figure \ref{algo:add_approx} (corresponds to alternative use of data, i.e., OR in the query):
%     \begin{enumerate}
%         \item $LV_1 \cup LV_2 = \{\vec v_1, \vec v_2, \vec v_3\} \cup \{\vec v_4\} = \{\vec v_1, \vec v_2, \vec v_3, \vec v_4\}$
%         \item $|\{\vec v_1, \vec v_2, \vec v_3, \vec v_4\}| = 4 > 3 = max\_vectors\_num$
%         \item $LV_3 = \operatorname{ClusterVectorsUsingKMeans}(\{\vec v_1, \vec v_2, \vec v_3, \vec v_4\}) = \{\vec c_1, \vec c_2, \vec c_3\}$\\
%         such that: 
%         \begin{equation*}
%             \vec c_1 = \begin{pmatrix} -0.75\\ 0.75 \end{pmatrix},
%             \vec c_2 = \begin{pmatrix} 1\\ 1 \end{pmatrix},
%             \vec c_3 = \begin{pmatrix} 0\\ -1 \end{pmatrix}
%         \end{equation*}
%         are the centroids of the three clusters.
%     \end{enumerate}
% \end{example-withrun}
% % multiplication
% \begin{example-withrun}
%     Let us follow the construction of $LV_3$, that represents the lineage embeddings of $t_1 \cdot t_2$ using the multiplication Algorithm in Figure \ref{algo:mul_approx} (corresponds to joint use of data, i.e., AND in the query):
%     \begin{enumerate}
%         \item $\operatorname{CartesianProduct}(LV_1, LV_2) = \{(\vec v_1, \vec v_4), (\vec v_2, \vec v_4), (\vec v_3, \vec v_4)\}$
%         \item $\{\operatorname{Avg}(\vec v_1, \vec v_4), \operatorname{Avg}(\vec v_2, \vec v_4), \operatorname{Avg}(\vec v_3, \vec v_4)\} = \{\vec a_1, \vec a_2, \vec a_3\}$\\
%         such that:
%         \begin{equation*}
%             \vec a_1 = \begin{pmatrix} -0.5\\ -0.25 \end{pmatrix},
%             \vec a_2 = \begin{pmatrix} 0.5\\ 0 \end{pmatrix},
%             \vec a_3 = \begin{pmatrix} -0.25\\ 0 \end{pmatrix}
%         \end{equation*}
%         are the average vectors, of each pair, in the cartesian product.
%         \item $|\{\vec a_1, \vec a_2, \vec a_3\}| = 3 \leq 3 = max\_vectors\_num$
%         \item $LV_3 = \{\vec a_1, \vec a_2, \vec a_3\}$
%     \end{enumerate}
% \end{example-withrun}
% % Algorithm - use Algorithm2e
% \input{algorithms/init_approx_prov.tex}
% % \footnotetext{The square brackets notation [ ] constructs a list, analogous to set construction. The order of words in the list is arbitrary.}
% \input{algorithms/add_approx_prov.tex}
% \input{algorithms/mul_approx_prov.tex}
% \input{algorithms/provsql_approx.tex}
% % \input{algorithms/formula_approx.tex}


% % \footnotetext{Perform K-Means clustering over the vectors into \textit{max\_vectors\_num} groups and return the centers of each group.}

% % Explain the motivation and process of approx provenance querying

% \subsection{Lineage querying} As was mentioned in section \ref{sec:word_embeddings_intro}, word embeddings (i.e., vectors) provide insights into the meaning of words via a word-word similarity score. The similarity measure is the cosine distance between the word vectors. Our goal is to construct and maintain lineage embeddings, which can provide insights to the reasons for the existence of tuples via a tuple-tuple similarity score, which is analogous to word-word similarity score (the tuples of interest are the ones generated by a query, which in turn use previously query-generated tuples as well as explicitly inserted ones).
% \par Consequently, given a tuple and its lineage embeddings, we can calculate the pair-wise similarity against every other tuple in the DB (or a subset, e.g., in a specific table) and return the top $N$ (a parameter) most lineage-similar tuples (these resemble a subset of the lineage \cite{Cui:2000:TLV:357775.357777}). 
% There are many algorithms for approximate vector search, e.g., based on LSH \cite{lsh}. Approximate vector search is a very active area of research and we can utilize known algorithms (see, e.g., \cite{sugawara-etal-2016-approximately}).
% Due to the reliance of lineage embeddings on an underlying statistical ML model, we expect our produced lineage to \textbf{approximate} the exact \textbf{lineage}.\\

% % \footnotetext{Call this algorithm via $+(LV1,LV2)$ or using the infix notation $LV1 + LV2$.}
% % \footnotetext{Call this algorithm via $\cdot(LV1,LV2)$ or using the infix notation $LV1 \cdot LV2$.}

% % indtended usage clarification
% \subsection{Intended usage} The intended usage of lineage vectors is as follows:
% \begin{itemize}
%     \item Each manually inserted tuple - has a set consisting of a single tuple vector.
%     \item Each tuple in the result of a query - has a set of up to $max\_vectors\_num$ tuple vectors.
%     \item When calculating similarity - we always compare between two sets of lineage vectors, by using the formula we shall present in section \ref{sec:latent_wv_model}.
% \end{itemize}
% % Show how our approach addresses the problems listed in the motivation section
% \subsection{Comparison to exact lineage} In section \ref{sec:motivation_approx} we listed some problems that arise when approaching distant (i.e., over the whole DB history) lineage computation with traditional techniques. Our proposed solution addresses these problems as follows:
% \begin{itemize}
%     \item Lineage embeddings require only a constant space per-tuple.
%     \item Lineage embeddings are immutable once computed, and do not get more complex over time (querying time depends only on the number of tuples, with which we compare the target tuple, i.e., the one to be explained via lineage).
%     \item Returned explanations (produced lineage) are simply the top-N justifications, and may assist in further analysis.
% \end{itemize}
% We approximate lineage using embedding techniques. As noted, lineage is a type of provenance, and it can be expressed using semirings. Now, semirings can also express how-provenance, which is more general and informative than lineage, but, it becomes extremely complex to track as histories develop. Thus, lineage seems to be a more practical tool for analysts.
% A major advantage of our approach is realized for distant provenance. A process that is equivalent functionally to recursive drill through is done automatically (i.e., if we envision the distant provenance of a tuple as a recursive tree-like structure).\\

% \par After developing the concept of lineage embeddings in isolation, it is essential to test it as part of a real-world implementation. We developed a Python module that adds ``lineage via embeddings" capabilities to a Postgres DBMS, by integrating our module with the ProvSQL \cite{Senellart2018} extension. The construction of the lineage vectors is based on the algorithms presented above.


\section{Word Vectors Model}\label{sec:latent_wv_model}
% All logic and vector training from db code is written in python (detached from ProvSQL, enabling faster development)
The system we propose is based on a word vectors model. Such a model is composed of a collection of real-valued vectors, each associated with a relevant DB term. The process of deriving vectors from DB-derived text is called \textit{embedding}. \\
% Word Vectors Training
\subsection{Training word vectors} 
As in Bordawekar et al. \cite{DBLP:journals/corr/BordawekarS16}, we train a \textsc{Word2Vec} model \cite{rehurek_lrec} on a corpus that is extracted from the relevant DB.
A naive transformation of a DB to unstructured text (a sequence of sentences) can be achieved by simply concatenating the textual representation of the different columns of each tuple into a separate sentence. This approach has several problems \cite{DBLP:journals/corr/BordawekarS16}:
\begin{itemize}
    \item When dealing with natural language text, there is an implicit assumption that the semantic influence of a word on a nearby word is inversely proportional to the distance between them. However, not only that a sentence extracted from a tuple does not necessarily correspond to any natural language structure, but, it can be actually thought of as ``a bag of columns"; i.e., the order between different columns in a sentence has no semantic implications.
    \item All columns are not the same. That is, some columns-derived terms may hold more semantic importance than others in the same sentence (generated from a tuple). For instance, a primary key column, a foreign key column, or an important domain-specific column (e.g., a manufacturer column in a products table). This implies that in order to derive meaningful embeddings from the trained word vectors, we need to consider inter-column discrimination, during both training and evaluation phases. This topic is further discussed in chapter \ref{chap:per_tuple_lineage_vectors}.
\end{itemize}
We solve these problems by properly setting hyperparameters (e.g., window size), artificially (and carefully) injecting repeated text inside a sentence, and dividing the training into multiple, different stages. 
A detailed outline of the ``textification" and training processes follows:
\begin{enumerate}
    \item Generate a \texttt{Key} column with unique values for each table in the DB.
    \item Extract two corpora from the DB, one with columns (except the \texttt{Key} columns) as sentences, and the other with tuples as sentences.
    \item Transform each numerical value in the generated corpora by concatenating the relevant column name to the number. Other methods of dealing with numerical values exist as well, and are discussed in chapter \ref{chap:discussion}.
    \item Transform each sentence in the generated corpora by injecting the corresponding \texttt{Key} value repeatedly after every $k$ (a separate hyperparameter for the columns corpus and the tuples corpus) words in the sentence. The idea is to encode more information about each tuple in its corresponding \texttt{Key} value vector.
    \item Incrementally train a \textsc{Word2Vec} model on the generated corpora. First, train on the columns corpus and then on the tuples corpus. One may question why we have the same model for both the tuple-based (see chapter \ref{chap:per_tuple_lineage_vectors}) and the column-based (see chapter \ref{chap:per_column_lineage_vectors}) lineage vectors. The answer is that experimentally the models that are restricted to only the tuples corpus or only the columns corpus yield inferior results.
\end{enumerate}
One more interesting caveat, of dealing with texts extracted from relations, is that missing data (empty/null cells) is sometimes automatically converted (when an external tool, e.g., Python, is used to interface with PostgreSQL) to `None'. Given that `None' is not a \textit{stop word} (words that are removed from a corpus as part of a common practice of cleanup before training), we realized that `None' adds noise to the vectors (if there is a lot of missing data in the DB). Adding `None' to the list of \textsc{Word2Vec} stop words improved drastically both the word embedding model quality and the overall accuracy of our system. \\

\par The quality of the word vectors model is crucial to the success of our system. However, optimizing the overall performance should focus not only on the training phase, but also on the way we utilize the trained model. Next, we show a number of such optimizations.\\
% sentence embeddings
\subsection{Sentence embeddings}\label{sec:sentence_embeddings} Extracting sentence embeddings from text has been a well-researched topic in the NLP community over the last seven years. State-of-the-art pre-trained models (e.g., Universal Sentence Encoder \cite{Cer2018UniversalSE, use_github} and BERT \cite{devlin2018bert}) are trained on natural language texts, and thus are not suitable for sentences generated from relational tuples (see discussion above). Hence we train a word embedding model and infer the sentence vectors as a \textit{function of the set of word vectors containing all the words} in a sentence. We average the word vectors for each column separately, and then apply weighted average on the ``column vectors" (the weight is based on the relative importance of a column, as discussed above). As will be shown, column-based vectors result in significant improvements to lineage encoding.
% comparing two sets of vectors
\subsection{Similarity calculation}\label{sec:similarity_calculation} Given two word vectors, the similarity score is usually the cosine distance between them. In our system, we want to calculate the similarity between the lineage representations of two tuples/columns (see chapters \ref{chap:per_tuple_lineage_vectors} and \ref{chap:per_column_lineage_vectors}, respectively); in both cases, the tuple or column, is associated with a \textit{set} of lineage vectors. That is, we need to calculate the similarity between \textit{two sets of vectors}\footnotemark. The ``logic" behind the following formula is balancing between the ``best pair" of vectors (in terms of similarity) in the two sets and the similarity between their average vectors:\\
\begin{equation*}
    \operatorname{sim}(A, B) = \frac{w_{max} \cdot max(ps) + w_{avg} \cdot avg(ps)}
    {w_{max} + w_{avg}}\\
\end{equation*}
\footnotetext{We note that \cite{
DBLP:journals/corr/BordawekarS16, 
DBLP:journals/corr/abs-1712-07199,
DBLP:conf/sigmod/BordawekarS17}
have also used various similar methods for measuring similarity between two sets of vectors.}
where $ps$ is the set of pair-wise similarities between a pair of vectors, one taken from set $A$ and one taken from set $B$. $w_{max}$ and $w_{avg}$ are (user-specified) hyperparameters. $max$ and $avg$ are functions that return the maximum and average values of a collection of numbers, respectively\footnotemark.
\footnotetext{This logic holds for both tuple-based vectors and column-based vectors (i.e., for each column separately).}


\section{Related Work}
% Relational Embeddings
\textbf{Relational embedding} is a very active area of research \cite{DBLP:journals/corr/abs-1803-01384, sigmod2020_keynote, pie2020_keynote}.
When converting a relational DB to unstructured text (see section \ref{sec:latent_wv_model}) special care is required to support numerical values correctly. Bordawekar and Shmueli \cite{DBLP:journals/corr/BordawekarS16} take the route of tokenizing numerical values from the DB by preceding each number with a ``range designator" (e.g., 1-10, 50-100, SMALL, BIG, etc.) in the generated corpus. In \cite{DBLP:journals/corr/abs-1712-07199} Bordawekar, Bandyopadhyay and Shmueli use clustering techniques to represent numerical values textually. In contrast, in this work we deal with numerical values by concatenating the relevant column name to each number in the generated corpus. This way, we ensure our model separates between numerical values of different fields, as they are conceptually different, semantically speaking (for example, we want the value 300 for a \texttt{nutrient\_code} field in a \texttt{nutrients} table to have a different learned vector than the value 300 for a \texttt{household\_serving\_size} field in a \texttt{serving\_size} table, from the USDA BFPDB \cite{usda_bfpd-dataset} dataset). In the specific database example with which we experimented, this simple technique proved sufficient.


% \section{Interim Findings and Hurdles}
% We conducted a series of experiments (see chapter \ref{chap:experimental_evaluation}) and came to the following interim conclusions:
% \begin{itemize}
%     \item Our explanations encode the text and not the tuples. Thus, relations with many similar tuples (text-wise) result in our system performing poorly. We assume that most of the problems listed below are symptoms of the same root cause.
%     \item Analysis of direct provenance (i.e., \textit{direct} explanations for the existence of tuples in a query result in terms of existing DB tuples, regardless of their historical raison d'être) on simple, complex (ones that might involve multiple tables with non-trivial joins and non-trivial ``where" conditions) and composite (that contain sub-queries) queries gives good results ($> 90\%$ accuracy for ``top $50\%$'' of the tuples in the lineage) generally. However, results suffer from inconsistencies sometimes, identifying tuples incorrectly as part of the linaege due to thier textual similarity to actual lineage tuples.
%     \item Distant provenance provides useful results. However, it suffers from the same problem of textual closeness.
% \end{itemize}

% \par In the next chapter we introduce methods that are designed to overcome these defficiencies.