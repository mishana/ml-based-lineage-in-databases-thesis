\chapter{Discussion}
\label{chap:discussion}


% % Relational Embeddings
% \section{Relational embedding} is a very active area of research \cite{DBLP:journals/corr/abs-1803-01384, sigmod2020_keynote, pie2020_keynote}.
% When converting a relational DB to unstructured text (see section \ref{sec:latent_wv_model}) special care is required to support numerical values correctly. Bordawekar and Shmueli \cite{DBLP:journals/corr/BordawekarS16} take the route of tokenizing numerical values from the DB by preceding each number with a ``range designator" (e.g., 1-10, 50-100, SMALL, BIG, etc.) in the generated corpus. In \cite{DBLP:journals/corr/abs-1712-07199} Bordawekar, Bandyopadhyay and Shmueli use clustering techniques to represent numerical values textually. In contrast, in this work we deal with numerical values by concatenating the relevant column name to each number in the generated corpus. This way, we ensure our model separates between numerical values of different fields, as they are conceptually different, semantically speaking (for example, we want the value 300 for a \texttt{nutrient\_code} field in a \texttt{nutrients} table to have a different learned vector than the value 300 for a \texttt{household\_serving\_size} field in a \texttt{serving\_size} table, from the USDA BFPDB dataset). 

% Following up on section \ref{sec:latent_wv_model} it should be noted that we train word embeddings for both textual and numerical columns and values, when . Numeric types for example require special care during training (we draw inspiration from the work on CI Queries of Bordawekar and Shmueli \cite{DBLP:journals/corr/BordawekarS16}; they use the term \textit{db2vec}). The distinction in strategy between different data types is done only in the preparation step to training the word embeddings model. 

% % Provenance Vectors
% Following up on section \ref{sec:proposed_solution_approx} we would like to clarify the intended usage of the \textbf{lineage vectors}. When using tuple-based provenance vectors:
% \begin{itemize}
%     \item Each manually inserted tuple - has a set of 1 tuple vectors.
%     \item Each tuple in the result of a query - has a set of up to $max\_vectors\_num$ tuple vectors.
%     \item We always compare between two sets of tuple vectors using the formula in section \ref{sec:latent_wv_model}.
% \end{itemize}
% When using column-based provenance vectors:
% \begin{itemize}
%     \item Analogous to tuple vectors but everything is done at the column level (see section \ref{sec:column-vectors}).
% \end{itemize}

% % Bloom Filters and timestamps
% \section{Bloom-Filters and Timestamps} Bloom-Filters and timestamps are universal enhancements that can be applied to any lineage-approximation technique. They complement beautifully the word embedding based lineage vectors, but are far from being a ``silver bullet" on their own.
% In particular, in the extreme case where most (or even all) the tuples of a table \texttt{A} participated in the evaluation of a query $q$, the Bloom Filters (almost) will not help when comparing a tuple from the result of $q$ to tuples from table \texttt{A}.

% % Natural Ranking
% \section{A Natural Ranking of Explanations}
% Our method implicitly produces a ``natural ranking" of explanations. It is unclear how to get this kind of ranking from semiring provenance polynomials (it might require significant additional manual work). To this end, there is a work by Deutch et al. \cite{Deutch2015} that ranks derivation trees for Datalog programs. In contrast, we deal with simpler SQL queries and we look at a much simpler notion: lineage, providing a succinct good-enough and useful one.